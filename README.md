# ğŸ§  Motor de Explicabilidade de Risco com IA (XAI para SOC)

Este projeto demonstra a aplicaÃ§Ã£o de Explainable AI (XAI) em um contexto de Security Operations Center (SOC).

Ao invÃ©s de apenas gerar um score de risco, o sistema explica **por que** um indicador foi classificado como ameaÃ§a, aumentando transparÃªncia, auditabilidade e confianÃ§a nas decisÃµes automatizadas.

---

## ğŸš€ Funcionalidades

* ClassificaÃ§Ã£o de risco (baixo â†’ crÃ­tico)
* ExplicaÃ§Ãµes interpretÃ¡veis (human-readable)
* NÃ­vel de confianÃ§a da decisÃ£o
* Score breakdown por fator de risco
* Rastreamento de requisiÃ§Ãµes (UUID + timestamp)
* Sistema de logging simples
* API pronta para integraÃ§Ã£o com n8n / SOAR

---

## ğŸ§  Arquitetura

```
ai-security-explainability-lab/
â”‚
â”œâ”€â”€ app.py                # API principal (Flask)
â”‚
â”œâ”€â”€ engine/
â”‚   â”œâ”€â”€ explainer.py     # geraÃ§Ã£o de explicaÃ§Ãµes
â”‚   â”œâ”€â”€ rules.py         # cÃ¡lculo de score e breakdown
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py        # persistÃªncia de logs
â”‚
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”„ Fluxo da AplicaÃ§Ã£o

1. Recebe dados via endpoint `/explicar`
2. Processa as features de risco
3. Calcula classificaÃ§Ã£o e nÃ­vel de confianÃ§a
4. Gera explicaÃ§Ãµes interpretÃ¡veis
5. Retorna resposta estruturada
6. Registra log da requisiÃ§Ã£o

---

## ğŸ“¥ Exemplo de Entrada

```json
{
  "risk_score": 85,
  "features": {
    "abuse_score": 90,
    "malicious_reports": 12,
    "geo_risk": "high"
  }
}
```

---

## ğŸ“¤ Exemplo de SaÃ­da

```json
{
  "request_id": "uuid-exemplo",
  "timestamp": "2026-02-23T12:00:00",
  "classificacao": "crÃ­tico",
  "risk_score": 85,
  "confianca": "alta",
  "explicacao": [
    "Score alto de abuso contribuiu significativamente para o risco",
    "MÃºltiplos reports maliciosos aumentaram a confianÃ§a da ameaÃ§a",
    "Origem geogrÃ¡fica de alto risco impactou a pontuaÃ§Ã£o"
  ],
  "score_breakdown": {
    "abuse_score": 40,
    "malicious_reports": 24,
    "geo_risk": 20
  },
  "impacto": "Este indicador apresenta risco com base em sinais correlacionados de ameaÃ§a"
}
```

---

## âš™ï¸ Como Executar

```bash
pip install -r requirements.txt
python app.py
```

---

## ğŸ”Œ Endpoint

**POST** `/explicar`

---

## ğŸ§ª Teste rÃ¡pido (curl)

```bash
curl -X POST http://127.0.0.1:5000/explicar \
-H "Content-Type: application/json" \
-d '{
  "risk_score": 85,
  "features": {
    "abuse_score": 90,
    "malicious_reports": 12,
    "geo_risk": "high"
  }
}'
```

---

## ğŸ§  Casos de Uso

* AutomaÃ§Ã£o de SOC
* Explainable AI (XAI) aplicada Ã  seguranÃ§a
* Auditoria de decisÃµes automatizadas
* Enriquecimento de threat intelligence
* GovernanÃ§a de IA

---

## ğŸ’¡ Diferencial

Este projeto vai alÃ©m da detecÃ§Ã£o de ameaÃ§as:

* Explica decisÃµes automatizadas de forma clara
* Aumenta a confianÃ§a em sistemas baseados em IA
* Facilita auditoria e compliance
* Reduz o efeito "caixa-preta" em seguranÃ§a cibernÃ©tica

---

## ğŸ›¡ï¸ Por que isso Ã© importante?

Sistemas modernos de seguranÃ§a baseados em IA nÃ£o podem ser "caixa-preta".

Este projeto demonstra como tornar decisÃµes:

* transparentes
* explicÃ¡veis
* auditÃ¡veis

Esses sÃ£o requisitos fundamentais em ambientes corporativos e regulados.

---

## ğŸ”® PrÃ³ximos Passos

* IntegraÃ§Ã£o com LLM para explicaÃ§Ãµes mais avanÃ§adas
* IntegraÃ§Ã£o direta com n8n
* Dashboard de visualizaÃ§Ã£o de decisÃµes
* PersistÃªncia em banco de dados (ex: PostgreSQL)

---

## ğŸ‘©â€ğŸ’» Autora

**Paula Sabino**
ğŸ”— GitHub: https://github.com/Paula-Tech007
